# -*- coding: utf-8 -*-
"""Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1myXM7klWrvwe_yO8iWrPQOJeS_9C39x4
"""

!pip install streamlit pandas matplotlib wordcloud plotly seaborn nltk
!apt install cloudflared

import nltk
nltk.download('vader_lexicon')
nltk.download('stopwords')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# import plotly.express as px
# import nltk
# from nltk.sentiment import SentimentIntensityAnalyzer
# from collections import Counter
# from nltk.corpus import stopwords
# import string
# import os
# 
# # Set page config
# st.set_page_config(page_title="Sentiment Analysis Dashboard", page_icon="📊", layout="wide")
# 
# # Initialize VADER Sentiment Analyzer
# sia = SentimentIntensityAnalyzer()
# 
# # Custom Sentiment Keywords
# POSITIVE_KEYWORDS = {"good", "great", "amazing", "love", "excellent", "fantastic", "awesome", "best", "wonderful", "perfect", "happy", "satisfied", "positive", "enjoy"}
# NEUTRAL_KEYWORDS = {"okay", "fine", "average", "neutral", "normal", "decent", "fair", "standard", "moderate", "acceptable"}
# NEGATIVE_KEYWORDS = {"bad", "worst", "terrible", "horrible", "poor", "awful", "hate", "slow", "problem", "issue", "disappointed", "negative", "unsatisfied"}
# 
# # Stopwords & Punctuation Removal
# STOPWORDS = set(stopwords.words('english')) | set(string.punctuation)
# 
# # Title
# st.markdown("<h1 class='stHeader'>📊 Sentiment Analysis Dashboard</h1>", unsafe_allow_html=True)
# st.write("This app provides **accurate sentiment analysis**, ensuring well-structured visualizations.")
# 
# # File Upload Section
# st.sidebar.header("📂 Upload Your CSV File")
# uploaded_file = st.sidebar.file_uploader("Choose a CSV file", type="csv")
# 
# if uploaded_file is not None:
#     data = pd.read_csv(uploaded_file)
# elif os.path.exists("/content/your_dataset.csv"):
#     data = pd.read_csv("/content/your_dataset.csv")
# else:
#     st.sidebar.warning("⚠️ Please upload a CSV file first.")
#     st.stop()
# 
# # Drop unwanted columns
# columns_to_remove = ["Unnamed: 0", "review_id", "app_id"]
# data = data.drop(columns=[col for col in columns_to_remove if col in data.columns], errors="ignore")
# 
# # Check required columns
# if 'content' in data.columns and 'score' in data.columns:
#     st.sidebar.success("✅ File successfully uploaded!")
# 
#     # Shuffle and limit dataset to 5000 rows
#     if len(data) > 5000:
#         data = data.sample(5000, random_state=42)
# 
#     # Sentiment Analysis (Score + VADER)
#     def analyze_sentiment(text, score):
#         if pd.isna(text) or text.strip() == "":
#             return "Neutral", 0
# 
#         # Use Score if available
#         if pd.notna(score):
#             score = float(score)
#             if score > 3:
#                 return "Positive", score
#             elif score == 3:
#                 return "Neutral", score
#             else:
#                 return "Negative", score
# 
#         # Otherwise, use VADER
#         vader_score = sia.polarity_scores(text)['compound']
#         if vader_score > 0.05:
#             return "Positive", vader_score
#         elif vader_score < -0.05:
#             return "Negative", vader_score
#         else:
#             return "Neutral", vader_score
# 
#     # Apply Sentiment Analysis
#     results = data.apply(lambda row: analyze_sentiment(row['content'], row['score']), axis=1)
#     data[['Sentiment', 'Sentiment Score']] = pd.DataFrame(results.tolist(), index=data.index)
# 
#     # 📌 **Display DataFrame Preview**
#     st.markdown("<h3 class='stSubheader'>📌 Data Preview</h3>", unsafe_allow_html=True)
#     st.dataframe(data.head(5))  # Display only the first 5 rows
# 
#     # 📌 **Sentiment Summary**
#     st.markdown("<h3 class='stSubheader'>📌 Sentiment Summary</h3>", unsafe_allow_html=True)
#     sentiment_counts = data['Sentiment'].value_counts()
#     st.write(f"**Total Feedback Items:** {len(data)}")
#     st.write(f"✅ **Positive Feedback:** {sentiment_counts.get('Positive', 0)}")
#     st.write(f"⚪ **Neutral Feedback:** {sentiment_counts.get('Neutral', 0)}")
#     st.write(f"❌ **Negative Feedback:** {sentiment_counts.get('Negative', 0)}")
# 
#     # 📊 **Sentiment Distribution Charts**
#     col1, col2 = st.columns(2)
# 
#     # Pie Chart
#     col1.markdown("<h3 class='stSubheader'>📌 Sentiment Distribution</h3>", unsafe_allow_html=True)
#     fig_pie = px.pie(names=sentiment_counts.index, values=sentiment_counts.values, title="Sentiment Breakdown")
#     fig_pie.update_traces(textinfo="percent+label")
#     col1.plotly_chart(fig_pie, use_container_width=True)
# 
#     # Sentiment Score Histogram
#     col2.markdown("<h3 class='stSubheader'>📌 Sentiment Score Distribution</h3>", unsafe_allow_html=True)
#     fig_hist = px.histogram(data, x="Sentiment Score", nbins=30, title="Distribution of Sentiment Scores")
#     col2.plotly_chart(fig_hist, use_container_width=True)
# 
#     # 📌 **Most Frequent Sentiment Keywords**
#     st.markdown("<h3 class='stSubheader'>📌 Most Frequent Sentiment Keywords</h3>", unsafe_allow_html=True)
#     all_keywords = []
#     for index, row in data.iterrows():
#         words = row['content'].split()
#         keywords = [word.lower() for word in words if word.lower() in POSITIVE_KEYWORDS | NEUTRAL_KEYWORDS | NEGATIVE_KEYWORDS]
#         all_keywords.extend(keywords)
# 
#     keyword_counts = Counter(all_keywords)
#     common_keywords = pd.DataFrame(keyword_counts.most_common(10), columns=['Keyword', 'Count'])
# 
#     fig_keyword_bar = px.bar(common_keywords, x='Keyword', y='Count', title="Top 10 Sentiment Keywords",
#                              color='Keyword', color_discrete_sequence=px.colors.qualitative.Safe)
#     st.plotly_chart(fig_keyword_bar, use_container_width=True)
# 
#     # 📌 **Word Clouds for Sentiments (Strict Filtering)**
#     st.markdown("<h3 class='stSubheader'>📌 Word Clouds by Sentiment</h3>", unsafe_allow_html=True)
# 
#     def extract_strict_keywords(text, sentiment):
#         words = [word.lower() for word in text.split() if word.lower() not in STOPWORDS]
#         return [word for word in words if word in (POSITIVE_KEYWORDS if sentiment == "Positive" else
#                                                    NEUTRAL_KEYWORDS if sentiment == "Neutral" else NEGATIVE_KEYWORDS)]
# 
#     for sentiment in ['Positive', 'Neutral', 'Negative']:
#         st.subheader(f"{sentiment} Sentiment Word Cloud")
#         sentiment_data = data[data['Sentiment'] == sentiment]['content'].dropna()
#         filtered_text = ' '.join([' '.join(extract_strict_keywords(text, sentiment)) for text in sentiment_data])
# 
#         if filtered_text:
#             wordcloud = WordCloud(width=800, height=400, background_color="white").generate(filtered_text)
#             plt.figure(figsize=(10, 5))
#             plt.imshow(wordcloud, interpolation="bilinear")
#             plt.axis("off")
#             st.pyplot(plt)
#         else:
#             st.write(f"No {sentiment} feedback available.")
# 
#     # 📥 **Download Button**
#     st.download_button("📥 Download Processed Data", data.to_csv(index=False), "sentiment_analysis.csv", "text/csv")
#

import os

# Start Streamlit in the background
os.system("streamlit run app.py &")

# Use cloudflared to expose the Streamlit app
!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared
!chmod +x cloudflared
!./cloudflared tunnel --url http://localhost:8501